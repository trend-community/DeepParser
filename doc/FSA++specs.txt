Next-generation NLP Oriented FSA++ Formalism Specs

The following is a definition of the FSA++ formalism. 

(1) Grammar Syntax
 
An FSA++ grammar consists of a number of pattern matching rules.
 
All rules are in disjunctive relationship and will be compiled into a uniform FSA++ network. 

If an fsa++ grammar has 50 rules Rule1, Rule2 .. Rule50, they are always OR-ed before being compiled into a unified big FSA network, 
there is no need for explicitly listing them like the following:

GrammarFSA = {Rule1|Rule2|...|Rule50}  // no need to list all rules in the grammar, system will OR them in parsing and compiling. 

Each rule starts with a rule name in the above format; a set of rules form a grammar inside one file, illustrated below:

SampleRule1(Top) = {.....};   						// a top priority rule
SampleRule1a(Top >SampleRule1) = {......};  		// also a top priority rule, and its priority is greater than the other top priority rule SampleRule1
SampleRule2 = <......>;    							// a rule with unmarked priority, priority less than Top rules but greater than Bottom default rules 
SampleRule3(>SampleRule2) = <<......>>;				// priority greater than SampleRule2 and otherwise it is unmarked between Top and Bottom
SampleRule4 = {......};								// unmarked priority between Top and Bot (Bottom)
SampleRule5(>SampleRule3 >SampleRule4) = {......};  	// priority greater than SampleRule3 and SampleRule4 and otherwise unmarked between Top and Bottom
SampleRule6(Bot) = {......};						// a bottom (Bot) level default rule
SampleRule6a(Bot >SampleRule6) = {......};		// a bottom level default rule, and its priority greater than the other bottom rule SampleRule6


(2) Rule Syntax

An FSA++ rule consists of a Rule Name, optionally followed by rule parameters inside (…), linked by = with the Rule Body which ends in semi-colon. 

Following the rule is unit tested samples after the keyword TEST, as illustrated below:

PassiveSimpleING = <"being|getting" [RB:^.R]1 [VBN|ED:VG Passive Simple Ing ]>;
TEST: Seeing and being seen are her pursuit. >> being seen:VG 

In the above rule, PassiveSmpleING is the Rule Name, the Rule Body is inside {…}; or <...>; and the unit tested sample is after TEST colon, 
with the input and output linked by >>. By convention, we name rules using Initial uppercase word-string for general rules, e.g. SimplePresent. 

The equal sign = links the rule name part with the pattern.  

{...} holds a pattern or a sub-pattern or any elements of a pattern when scope specification is needed to disambiguate. 
Therefore a pattern rule may involve several embedded {...} pairs. 

{{subpattern1} {subpattern2}};

[feature1 feature2|feature3]
==
[{feature1 feature2}|feature3]
<>
[feature1 {feature2|feature3}]

A pattern rule always ends with semi-colon, so {...}; or <...>; is scope for an entire pattern rule. 

When a pattern is put inside <…> which is syntax for chunking a pattern (called phrase), it is considered omitting {...}. In any case, 
semi-colon marks the end of a pattern rule. So:

PassiveSimpleING = <"being|getting" [RB:^.R]? [VBN|ED:VG Passive Simple Ing]>;
==
PassiveSimpleING = {<"being|getting" [RB:^.R]? [VBN|ED:VG Passive Simple Ing]>};
   
{[token-precontext]<phrase> {subpattern-postcontext}}; 
// this is a very typical rule, with one token for precontext and any number of tokens as postcontext (subpattern post context)

When a rule can be written in one line (including expert lexicon entry rules), the brackets and the colon {...}; can be omitted.  
 
<毫无+所+c1:V++ duiVt perF idiom4 Pred>
==
{<毫无+所+c1:V++ duiVt perF idiom4 Pred>};   

A linear pattern (e.g. the data structure for an input sentence) or a tree pattern (e.g. a parse tree representation of 
the processed sentence) consists of n available tokens.
  
[...] always represents a token (with the only exception when 2 or more tokens are supposed to be concatenated: in that case 
the result is one token so for clarity in writing, we can write something like this: ['which+one':...] == [which]+[one...].  
[Token] is an available element in the input string or an available node in the input tree 
(in fact, a graph of binary dependencies). 

When the scope is clearly marked, square brackets may be omitted, e.g. “word” uses double quotation marks and 'stem' uses single quotation marks:

"word"
== ["word"]

"this is a sentence"
== {"this" "is" "a" "sentence"}
== {["this"] ["is"] ["a"] ["sentence"]}

'stem' // matching both "stem" and "stems"
== ['stem']

'these be sentence' // can match "these are sentences".
== {'these' 'be' 'sentence'}
== {['these'] ['be'] ['sentence']}

['不禁'|!MD]
== (['不禁']|[!MD])
 
question mark as optional element suffix, or [optional-token]*1 or (optional-subpattern)*1 uses ?, *1 or 1:

[RB]?
== [RB]*1
== [RB]1

"in my opinion"?
== ("in my opinion")1
== ("in" "my" "opinion")*1
== (["in"] ["my"] ["opinion"])*1

If we use a feature to match a token, we cannot omit square brackets. Square brackets can only be omitted for "word" token 
or 'stem' token.

[feature]
 
{[feature1] "word" (feature2) 'stem'}
== {[feature1] ["word"] ([feature2]) ['stem']}

[] is a token with no condition, so it matches any token.

TEST is a Unit Tested case which defines input and output.  TEST does not participate in the pattern matching.
It is like comments after // and can be ignored in parsing (as well as compilation and running). 

The output in TEST will in time be updated with a readable output format from the processed unit case so that 
the system can automatically check its validity with the status-quo of the current system.  
For now, TEST is just like a user comment that the FSA parser and compiler simply ignore.

Rule Body

A rule body conceptually has two parts: FSA++ Pattern; Action (once pattern is matched).

(Action is often called 'side effects' of FSA++ pattern matching. 
There is also a mechanism in the community called FST, Finite State Transducer, 
which basically consists of an FSA++ accepting the input string and outputs another string based on 
some transduction actions defined in the rules.)

Action usually happens to specified tokens in the pattern, not to the entire pattern 
(internal representation of chunking a pattern into a phrase is arguably an action 
on the entire pattern depending on how to implement chunking). 
As a result, we can define the formalism where action commands are inserted to the related tokens inside the pattern.  
This eliminates most of the pointers marked for elements of a pattern, minimizing one major error source in coding. 
The pattern rule seems also more readable.

Of course, action only happens once the pattern is matched and that pattern wins over competing matched peer patterns.  
Therefore, in pattern matching stage, the action part of the pattern elements is initially ignored.

[Pattern Element : Action Format]

The syntax for delimiting a token condition and the action for this token is the colon : sign. 
Pattern is obligatory and action with the preceding colon are optional.
The colon sign stands for actions that need to happen to the matched token. For example,

NegSimpleIng = 
<@not [VBG:VG Negative Simple Ing ]>; 
TEST: Not understanding what's going on, he decided to leave. >> Not understanding

In this rule, there is an explicit action of feature update that needs to happen to the last matched verb token (HEAD of the chunk).  
Feature update refers to assigning or removing a feature associated with a token.

A feature in the action part after : is to assign the feature and the minus sign - as suffix is to remove the feature.  
In the above case, when this VG is formed, the head token VBG (ING verb) will be assigned 4 features: VG, Negtive, Simple, Ing, with or without spaces:

 [VBG:VG Negative Simple Ing]

Note that, both token condition and token action are placed inside square brackets [ ... : ... ], as an entire token description.
With the above design of mixing action with pattern elements (delimited by : inside a token syntax), 
a pattern is a rule and a rule is represented by a pattern.  In effect, pattern, rule and pattern rule are exchangeable terms in most cases.

Pattern Rule Without Action

There is a possibility of a pattern rule that involves no action at all for any elements.  Sometimes linguists can use these 
action-less rules as exception rules to prevent certain competing rules from taking some unwanted actions. 
(A debugger in the development environment should trace this explicitly to facilitate the linguists).

{"this is a pattern that involves no actions"};
TEST: this is a pattern that involves no actions >> 

Features in Rule Condition and Rule Action

These are features shown inside the Rule Body: 
(i) as token condition: [RB] (adverb), [VBN] (participle verb), [ED] (verb with -ed suffix); 
(ii) as action after colon of a pattern element, it involves token's feature assignment: 
VG (basic phrase verb group feature), Passive (passive form feature), Simple (simple aspect form feature), Ing (ING-VG).

Features before the - sign, e.g. f1 f2 f3- f4-, are for feature update action for the token, with feature assignment as default 
and feature removal using the suffix -.

Kernel Pattern and Context Checking in Rules

For all phrase-chunking pattern matching rules, we can add a one-token precontext and/or a sub-pattern postcontext (consisting of n tokens)
to the kernal pattern.  The addition of precontext and/or postcontext is regarded as a way of tightening the rule by enlarging the context.
So a rule can be tightened by extending the context in addition to adding more conditions to the kernal pattern elements. 
(Other linear pattern rules beyond <chunking> may in time also call for this context design if found useful and beneficial: 
we may consider using <<...>> as a way of scoping the kernal pattern.)

The kernel pattern for a chunking rule is inside <...> which represents the scope of a phrase.  There is always a one-token precontext 
[precontext] before the kernal pattern. If [precontext] is absent from the rule, the system adds one [] for you. This obligatory 
one-token precontext in effect means that the pattern matching when applying this rule always starts one token before the kernal pattern. 
If the kernal pattern attempts to match a phrase from the first word of a sentence, the matching actually starts with the precontext of 
one virtual token before the sentence: this virtual token is artificially added to the sentence data structure.  

PassiveSimpleING = <"being|getting" [RB:^.R]? [VBN|ED:VG Passive Simple Ing ]>;
==
PassiveSimpleING = {[] <"being|getting" [RB:^.R]? [VBN|ED:VG Passive Simple Ing ]>};

The one-token precontext design is motivated by enabling all rules to access possible conditions checking of precontext.
Without this, a kernal pattern without a pre-context will not have a chance to compete with the competing kernel patterns  
whose precontext is satisfied.  It is a common practice for a pattern to start with no pre-context condition. 
During debugging and development, we may want to tighten the rule by adding a precontext condition.  This precontext 
addition is intended to tighten the rule itself, and it not intended to exclude competition with candidate patterns.
The precontext design serves exactly the purpose of not excluding competition of candidate patterns.   

As for the post-context sub-pattern after the kernel pattern, it can be arbitrarily long:

PassiveSimpleING = 	{	<"being|getting" [RB:^.R]? [VBN|ED:VG Passive Simple Ing ]>
						[!NNS]
					};
==
PassiveSimpleING = 	{	[] 			// pre-context token
						<"being|getting" [RB:^.R]? [VBN|ED:VG Passive Simple Ing ]> // kernel pattern (for the scope of a chunked phrase)
						([!NNS]) 	// post-context subpattern
					};

The purpose of adding post context sub-pattern after the kernel pattern is to help tighten the rule without worrying about 
the Longest Matching Principle.  If the added post-context sub-pattern is regarded as part of the entire pattern rule, 
then the competing patterns without post-context will not have a chance to compete based on Longest Matching Principle. 
In order not to exclude the competing patterns, the post-context is defined as any sub-pattern after the kernel pattern 
(ending with > or >>).  This way, when the FSA++ runner applies these rules, the Longest Matching Principle will only 
consider the kernel pattern's length, disregarding post-context. This gives developers a lot of leverage in using context 
as a way of tightening the rules.  There are other ramifications of this context design which will be specified in time
when we elaborate on the FSA++ runner.

Note that neither the pre-context token nor the post-context sub-pattern is allowed to be associated with actions 
(after colon), context is only used as contextual pattern conditions (similar to feature checking as a condition), 
and actions can only be applied to the kernel pattern. (If an action is deemed required, make the "context" 
into the kernel pattern instead.) 

Use of Brackets in FSA++ Rule

In a pattern {...}; or a sub-pattern (...), SPACES (including any number of spaces, 
carriage returns and/or tab keys) are (used as) natural delimiters of token elements in the sequence. 

For example,

("this is a sample pattern all in words")

("this" 'be' "a sample pattern using words and stems")

("this" ["is":Predicate ] "a sample pattern with action")

{[DT] [JJ] [NN|NNS]}; // this is a pattern of three-token sequence, speecifying condition for POS features determiner+adjective+noun

We can add spaces or tab key to make the token boundaries clearer. Using the same samples, we have

{"this is a sample pattern"};
== {"this" "is" "a" "sample" "pattern"};
== {["this"] ["is"] ["a"] ["sample"] ["pattern"]};

{"this is a (sample) pattern"};
== {"this" "is" "a" ("sample") "pattern"};
== {["this"] ["is"] ["a"] (["sample"]) ["pattern"]};

{"this is a sample|classical|typical pattern"};
== {"this is a" "sample|classical|typical" "pattern"};
== {["this"] ["is"] ["a"] ["sample|classical|typical"] ["pattern"]};
== {"this" "is" "a" ["sample"|"classical"|"typical"] "pattern"};
== {["this"] ["is"] ["a"] {["sample"]|["classical"]|["typical]"} ["pattern"]};

{ 	"this"
	"is"
	"a"
	"sample"
	"pattern"
};
== {["this"] ["is"] ["a"] ["sample"] ["pattern"]};

These variations represent coding style differences, not syntax requirements of the formalism.

Note that (......) has two uses: (i) it is used to define a sub-pattern; (ii) it helps disambiguate the boundaries of any elements 
(features, tokens, or sub-patterns) when they are involved in logical AND, OR or NOT relationships.

When there are SPACES needed inside a token description, square brackets [....] should be used to make the boundaries between tokens clear, 
for example,:

[ ((AP|VG Good)|'可以') Fact !Neg|Neg2 clause ] 
// a fairly complicated token description for the purpose of illustration, not as a recommended style of coding: 
// positive adjective or verb phrases or the STEM 可以，plus some feature-checking

The following expressions are all equivalent:

("word") // the sub0pattern containing the token "word"
==(["word"])

["word"] // the token "word" 
== "word"

[token], <phrase>, (boundary-scope), and {rule}; 
 
Keep in mind that [...] only represents a token. When there is no ambiguity with SPACE as natural delimiter of token elements 
in a pattern, [] can be omitted. 

{DT []? A N}; // determiner + any-optional-token + adjective + noun
== {[DT] []? [A] [N]}; // Suffix ? represents an optional element in a pattern

Square brackets {pattern-rule}; (ending with semicolon) is used for the entire pattern rule scope. 

A rule pattern can embed sub-patterns or tokens inside like {... (...[...] ..) ...[...] ...}, 
but [...] cannot have embedded elements and it has to just scope the internal boundaries describing just one token.
In this sense, its use mainly serves style purposes for making the code clearer in making tokens stand out. 
When there is no ambiguity, its boundary function can actually be superseded by SPACES, {...}, (...), <...>, "...", '....'.

STEM and FEATURE

STEM can be mixed with FEATURE in use with or without explicit single quotation marks.  When mixed, every string is assumed to 
be a stem unless it is in the feature list. When a STEM literal has conflict with features, then use '...' (sort of like 
escape symbols).  If no conflict, whether to use '...' or not is a question of style and readability.

(CD sort of)
== ([CD] 'sort' 'of') 

More sample token descriptions are illustrated below:

!"word" // (negation)
==[!"word"]

['stem' !feature]  // (checking STEM and the negation of a feature) 

[@word|feature] // (OR relationship between a macro call and a token feature condition)
== ([@word]|[feature])

"word"?        // (optional word)
== ("word")*1 
== ("word")?
== ["word"]?

[feature]*3 // up to 3 times of this token in sequence 
== [feature]3

["word"|'stem'|feature] 		// logical OR 
== ("word"|'stem'|feature)
== (["word"]|['stem']|[feature])

(feature|"word")*3 			// OR and up to 3 times
== (feature|"word")3
== [feature|"word"]3

More sample (sub-)patterns are illustrated below:

{"this is a sample word string pattern"};  // SPACES inside quotation marks are token delimiters but SPACES inside [...] are logical AND
== {["this"] ["is"] ["a"] ["sample"] ["word"] ["string"] ["pattern"]};

{'this' 'be' 'a' 'sample' 'stem' 'pattern'}; 
== {'this be a sample stem pattern'};

([feature1] [feature2] [feature3]) 			// trigram  sub-pattern

("word1" 'stem2' [value3])

(^t1"word1" ^t2'stem2' ^t3[value3])
== (^t1["word1"] ^t2['stem2'] ^t3[value3])

{@tokenT1 (@subpattern)2 "word3"? 'stem4'3 value5};
== {@tokenT1 (@subpattern2)*2 "word3"*1 'stem4'*3 value5};
== {@tokenT1 (@subpattern2)*2 ["word3"]? ['stem4']3 [value5]};

Where there is potential ambiguity, use (...) to help disambiguate. (...) is for the internal use of scope brackets 
for sub-patterns or for any other occasions of disambiguation or for style.


 (3) Pattern Condition

Logical OR

The bar symbol | is used for disjunctive (logical OR) relation.

(DT A|NN N) // this is an NP pattern allowing either an adjective (A) OR a singular noun (NN) between a determiner (DT) and a noun (N)
== ([DT] [A|NN] [N])

Logical AND

SPACE is used for logical AND inside a token [...].

['be' VBN] // STEM:be AND FEATURE:VBN, matching "been"

([A] [N !V2]) // this is an NP pattern checking two tokens, an adjective (A) followed by a noun (N) which is NOT a candidate verb (!V2)

[(f1|f2 f3) (f4|f5 f6)]
== [(f1 | f2 f3) (f4 | f5 f6)]
== [((f1 | f2) f3) ((f4 | f5) f6)]

Note that space is allowed before or after logical OR (but not a preferred style); 
but as a prefixing symbol "!" (NOT), space is NOT allowed between the prefix and the item that gets negated. 


Logical NOT 

The exclamation mark ! is used for NOT, as shown in the sample pattern above (!V2).

[!NN|JJ]  	// this is an often-used and convenient short-hand: OR is before NOT. 
==[!(NN|JJ)]
== [!NN !JJ]
== [(!NN) (!JJ)]

STEM or WORD Checking

Single quotation marks indicate STEM checking while double quotation marks for WORD checking. 
Thus, "been" is checking WORD:been; 'be' is checking STEM:be; therefore "been" is equivalent to ['be' VBN].

Note that since we never use hanzi (Chinese characters) as feature names, when hanzi appears inside rule patterns,
they are treated as singleton STEM checking, even if s and the single quotations ([0 '...']) are omitted.  


{ ^占 [NP|DE:^O.M] [的|之:^O.X]? ^O[percent:^.O] };
== { ^[0 '占'] [NP|DE:^O.M] [0 '的|之':^O.X]? ^O[percent:^.O] };

<为 ([N|JJ|CD] 的?)? [N:^V.R]>
== <[0 为] ([N|JJ|CD][0 的]?)? [N:^V.R]> 

-之类? == [-之类]? == ['-之类']?

In fact, not only STEM single quotations for Chinese characters can be omitted, other STEM single quotations can also be omitted
to make rules more readable with less meta syntax (one goal for this formalism syntax is to reduce the use of meta symbols, including 
this type of omission as well as avoiding the use of embedded brackets).  STEM is distinguished from FEATURE in that features can always 
be looked up from Feature Table.  Only when there is a clash of a stem with a feature, then the STEM single quotations become obligatory.   

More examples are given below:

@linkVerb = 'be|seem';
@are = "are|am|im|is|\'s|seems|seem";
// This macro is named @are. The token 's is checked with an escape character \ as ' is reserved single quotation symbol for STEM checking.

NORM

In addition to WORD and STEM, we add NORM literal checking by using /norm/ to check NORM:norm.  NORM is a field in the data structure 
where a close set of synonyms will select a normalized representative (in a range <= 30 words, mostly < 10 words ):  
this is to help enhance recall with almost no compromising precision.  

@parents = /mother|father|parent/;  // /mother/ == 'mother|mom|mommy'; /father/ == 'father|dad|daddy'.
  
SPACE

SPACES that are not around reserved symbols are by default token delimiters in pattern syntax. 
Token here refers to on-delimiting symbols defined in this FSA++ language for NLP. 
By default, SPACE means that it represents token boundaries when there are no other explicit delimiters or parentheses around SPACES. 
For example, the bigram patterns below are equivalent: 
(of course there is difference in style and we prefer a coding style that is not confusing, more readable, 
and when there is possible confusion, encourage the use of (...) to make the boundaries explicit.)

( 	f1 | f2
	f3
)
== ( f1 | f2 f3 )
== ( (f1|f2) f3 )
== ( [f1|f2 f3] )
== ( [f1 f3]|[f2 f3] )

The SPACES around | (or around other reserved delimiting symbols) do not count as token delimiters. 

Literal checking in STEM, WORD or NORM can list the disjunctive literals inside the delimiting symbols, i.e.

"w1|w2|w3" 
== ["w1|w2|w3"] 
== ["w1"|"w2"|"w3"]
== (["w1"]|["w2"|["w3"])

The former is preferred because it looks cleaner and simpler.

Default Association 

The association between NOT and OR is illustrated below:

!f1|f2|f3 
== !(f1|f2|f3) 
== (!f1 !f2 !f3) 

!f1|f2|f3 is frequently used, so we get used to it and we do not usually use !(f1|f2|f3), e.g. 

!NNP|caseAb|caseAB (neither a proper name nor an upper case token)
== [!(NNP|caseAb|caseAB)]
== [!NNP !caseAb !caseAB]

The association between NOT and AND is same as that of Not and OR:

[!f1 f2 f3]
== [f2 f3 !f1]

Pragmatically the following is much more often used and much more useful in rule conditions:

[!f1|f2|f3]
== [!(f1|f2|f3)] 
== [!f1 !f2 !f3] 

Association between AND and OR:

[f1|f2 f3] == [(f1|f2) f3]   // the latter is preferred to make the code preference explicit
[f1 f2|f3] == [f1 (f2|f3)]   // OR has higher priority than AND inside a token description 

Kleen Star 

Kleen star *n is used as syntax for matching the pattern n number of times.

The use of Kleen star * in classical FSA or regular expression is not adopted in our FSA++ 
because it may run out of memory stack at run-time and also because there are no actual needs 
for infinite number of elements.  So our compiler will default * as to mean *3 (repeat up to 3 times).
Linguists are advised to specify *n when another number other than 5 is deemed appropriate.

([DT] [A]* [N]) 	
== ([DT] [A]3 [N])
== ([DT] [A]*3 [N])

Unlike most FSA formalisms, the use of the plus sign + for 1 or more times is not supported 
in this formalism because 
(i) there is no such need: the regex A+ can always be rewritten as (A* A) or (A A*); 
(ii) the plus sign is already used as suffix for feature assignment in action and 
we do not want to unnecessarily over-burden the symbol.

The form ([RB] A+) as an AP rule will trigger an error message due to the use of the undefined Plus sign here.

Almost all classical cases of Kleen star or Plus can be replaced by the use of the devise below for up-to-n 
or 1-to-n optional sub-patterns. Further more, kleen star * can be omitted when there is no ambiguity:

("very"9 [A])
== (["very"]*9 [A]}

Optional Elements Up to n: 

For up to n (n>1) number of optional sub-patterns (including single tokens), one can use the syntax (...)*n, 
for example:

([DT] [A]5 [N])
== ([DT] [A]*5 [N])
== ([DT] (((([A]? [A])? [A])? [A])? [A])? [N])
== ([DT] [A]? [A]? [A]? [A]? [A]? [N])
== ([DT] (A)? (A)? (A)? (A)? (A)? [N])
== ([DT] [A]*2 [A]*3 [N])
== ([DT] (A)2 (A)3 [N])
== ([DT] (A)2 (((A)? A)? A)? [N])

Note: [token]*n, (token)*n and [token]*n are treated the same as ([token])n.
 
([DT] [A]5 [N])
== ([DT] ([A])*5 [N])

For style's sake, we suggest always stick to (...)n instead of (...)*n.

For 2 to n times of pattern matching, the suggestion is just "repeat the sub-pattern another time" as shown below:

([DT] (A)3 [A] [N]) 	// A is matched 1 to 4 times
== ([DT] (((A)? A)? A)? [A] [N]) 
== ([DT] (A)? (A)? (A)? [A] [N])
== ([DT] [A]? [A]? [A]? [A] [N])

Optional Element

Unlike most FSA formalisms, the use of the round brackets (...) for optional element (0 or 1 time) is not supported 
in this formalism because 
(i) there is no such need: the regex (A) can always be rewritten as A? or (A)*1; 
(ii) (...) is already used for scope disambiguation and we do not want to unnecessarily over-burden it.

? or *1 is used for optional sub-pattern (including token) in pattern matching.

([DT] [A]? [A]? [N])

This is a typical NP pattern, starting with a determiner DT followed by up to 2 optional adjectives (A) 
and ends with a noun (N), covering cases like

"a pretty girl"
"a strong intelligent boy"

Pre-context, Kernel, Post-context in Pattern

The pattern condition contains Kernel at minimum. There is an optional precontext token and an optional post-context sub-pattern.  

{	[T0]  			// the precontext checks a feature for "token 0" (the artificial token as "beginning of sentence" before first word)
	<@not [VBG]> 	// the kernel pattern checks a negative adverb (by calling a macro) followed by an ING verb
	[!NNS] 			// the post－context checks a token which is not a plural noun (NNS)
};
 

(4) Actions After Pattern Matched

Action include: (i) chunk actions; (ii) feature update actions; (iii) binary (dependency) link actions.

In general, SPACE is used as delimiter between different actions after colon 
just like SPACE is used asdefault delimiters of pattern elements (rule tokens) before colon.  

In order to distinguish these two types of SPACE use, any token description 
involving both condition and action(s) (with colon in between) must use the 
token boundary delimiters namely square brackets [condition:actions].
In other words, only token descriptions without an associated action are allowed 
to omit square brackets when there are no ambiguities.

Chunk Actions 

Chunk actions involve: (i) chunk HEAD specification; (ii) chunk FOOT specification; 
(iii) chunk scope specification; and (iv) chunk type feature assignment action; 
(v) chunk internal binary (dependency) link action.

In chunking rules, the XP (basic phrases such as NE, NP, AP, VG, PP) scope is represented by the kernel pattern <xp>.

{ 	[BOS]  	// precontext 
	<@not [VBG:VG Negative Ing ]> 	
	[!NNS]
};
// the XP kernel pattern checks a negative adverb (by calling a macro) followed by an ING verb to form a VG chunk with three features assigned.

There are implicit actions that need to happen to all chunking rules with the syntax <...> for the kernel pattern xp, to be detailed below.

Chunking is to build basic phrases XP represented by the XP features: DI (Data Item), NE (Named Entity), 
NP (Noun Phrase, often called Base NP or BaseNP in the community), AP (Adjective Phrase), PP (Prepositional Phrase), 
VG (Verb Group, not VP which is another level construct), and RP (Adverb Phrase). They are represented as special features
in the data structure.  The phrase structures (borrowed from Phrase Structure Grammar) represented by these features 
are basic linguistic building blocks (sometimes called XPs) that serve as leaf nodes in a syntactic dependency 
tree structure, acting as syntactical roles in a sentence parse.

The subtypes of XP will be named like MoneyDI, TimeDI, DateDI, WeightDI, EmailDI, etc, maintained in a feature ontology.
The kernel pattern <...> defines a pattern of linear tokens with left and right boundaries.  While other actions can be 
associated within a token description after colon, chunking needs a token string as its object for representation.

Chunk Structure XP

There are implicit actions that need to happen to all chunking rules with the syntax <...>.  More specifically, 

(i) normal chunking (excluding Singleton XP below) involves removing FEATURE:0 (Xbar 0 according to X-bary Theory). 
0 is by default automatically assigned to every token in the beginning until it is turned off by XP chunking now. 
This is a special feature used for control of different ways of checking configuration of XP (see: Xbar 0 XP checking).

(ii) store the boundary information inside the data structure of the head token: the boundaries of a XP phrase 
are represented pointers to FIRST (the leftmost token) and LAST (the rightmost token) in addition to FOOT and HEAD: 
the default FOOT is equal to FIRST and the deffault HEAD is equal to LAST if there are no explicit ^H and ^F 
specification otherwise in the pattern rule (to be explained below).

(iii) Explicit HEAD and FOOT Pointers in Chunking: there are two special pointers ^H and ^F used as prefixes 
before a token description in Chunking stage for marking HEAD and FOOT of a phrase: they are reserved to tell 
the system which token in the pattern is HEAD or FOOT when the default positions for HEAD and FOOT need to be overridden. 
For example:

IsNotV(Top) = <^H[@are:VG Negative Simple Present ] (["both|all":^H.R]) ^F[@not] >;

This pattern rule says that the negative VG in cases like "are not" is different from other VG (e.g. "will not be") 
in that the HEAD token is the first token "are" (@are) and the FOOT token is the last token "not" (@not).

(iv) Xbar 0

Most XP consists of more than one tokens, hence the distinction between HEAD, FOOT, FIRST and LAST pointers inside a chunk structure. 
When only one token forms an XP, so-called Singleton XP or Xbar XP, the treatment in implementation is assigning 
the corresponding feature (NP, AP, etc.), with no FEATURE:0 removal, nor HEAD/FOOT/FIRST/LAST assignment.

Singleton XP can be formed in lexicon simply by adding the XP feature as well as by chunking rules. In the following rule,

quite_some = 
{	<["quite":^.Mod]? ["some":NP ]>
	[IN]
};
TEST: She spent quite some on fasion. >> quite some:NP
TEST: She only spent some on fashion, >> some:NP

"quite some" in the first TEST form a normal XP, removing FEATURE:0 and FOOT=FIRST="quite", HEAD=LAST="some" 
while "some" in the second TEST forms a Xbar 0 XP with only the feature update NP.

(v) Singleton XP Checking

By default, a token description [...] checks the token (if it is not a phrase) or the head of token (if it is a phrase).
But XP can also be a singleton phrase, meaning that XP is formed of only one token. There is a need to check a token 
that is either one token before XP chunking or a singleton XP.  For this purpose, the syntax is [0 ...], e.g.

[0 'work']
[0 NN|JJ]

(vi) Internal Linking Within a Chunk

More info on the use of ^pointer will be specified in "Binary Link Actions" Section.  In chunking rules, there is a special 
representation ^.relation that stands for linking this node to the head token of this phrase as relation.  
The head pointer does not have to be specified in the pattern if the head is the default last token in the pattern. For example,

NegSimplePresent = 
<'do|does' @not [RB:^.R]? [VB:VG Negative Simple Present ]>; 
TEST: she does not like the product >> does not like:Present Simple Negative 

In the above pattern rule, by [RB:^.R]?, we know there is an optional adverb token in the pattern, which, if present, 
is decoded to be adverbial (.R) of the unmarked head token (by default the last verb token VB).  Hence, 

<'do|does' @not [RB:^.R]? [VB:VG Negative Simple Present ]}; 
== <'do|does' @not [RB:^.R]? ^[VB:VG Negative Simple Present ]};
== <'do|does' @not (RB:^H.R)? ^H[VB:VG Negative Simple Present ]};

< [DT:^.Mod] [JJ:^.Mod]5 [NN] >;
== < [DT:^.Mod] [JJ:^.Mod]5 ^[NN] >;   // ^ is a nameless pointer that can be used when there is only one pointer needed
== < [DT:^H.Mod] [JJ:^H.Mod]5 ^H[NN] >;  // ^H is a reserved pointer to identify HEAD

(vii) Pattern of Multiple Chunks

When the kernel pattern involves more than one XP sub-patterns to form multiple basic phrases XPs 
(such chunking rules are few but necessary), multiple sub-patterns of the form (<xp1>...<xp2>...<xp3>) will appear.
The kernel pattern in this case starts from the first xp <... and ends with last xp ...>. 
 
{	<[City:locNE ]> 	// this sub-pattern checks the lexical feature City for city name and form a location chunk NEloc 
	[CMMA] 				// this checks comma
	<[State:locNE ]> 	// this sub-pattern checks the lexical feature State for state name and form a location chunk NEloc
};
＝＝
{	[] 					// pre-context
	<[City:locNE ]> 	// this sub-pattern checks the lexical feature City for city name and form a location chunk NEloc 
	[CMMA] 				// this checks comma
	<[State:locNE ]> 	// this sub-pattern checks the lexical feature State for state name and form a location chunk NEloc
	[]					// post-context
};

This rule forms two XP chunks (both are NE locations) in one kernel pattern, matching strings like "Cupertino, California".

(viii) Chunking a word or a compound word instead of XP phrase:

Chunking a compound, we use the same <coumpound word> syntax, but there is no XP feature.

Chunking a single word, we use <<word>> syntax which basically means anything beyond is only context.  This <<word:f1 f2...>> 
syntax is often used in Expert Lexcions to update features by checking certain context.  After features are updated (in effect 
a normalization or disambiguation process), the token is supposed to follow the general rules in later modules.

Note the difference between a singleton XP chunk (bar 2 according to X-bar theory), a non-XP compound chunk (bar 1) 
and a non-XP word "chunk" (baer 0: in fact it is not a chunk per se any more, we simply make use of chunk syntax.  

(ix) Default Head path checking [1...] vs. Default Singleton checking [0...]

By default, feature checking is head-path checking, meaning that it will attempt to match the feature of a word or 
the feature of the head word of an XP, e.g.

[human] == [1 human], it will match both 'person' and ^NP['the old person'].

If we want it to match only the singleton token instead of the head of an mXP (multi-word XP), we use explicit 
singleton matching syntax [0 human].  [0 human] will match only 'person' and ^NP['man'], but not ^NP['the old man'].

By default, STEM, WORD and NORM checking is singleton checking instead of head path checking, meaning that it will 
only attempt to match a word with FEATURE:0, not the head word of an an mXP, e.g.

['person'] == [0 'person'], it will only match 'person' and will not match ^NP["the old person"].

Then what do we do when we want it to match the head path word too?  We use [1 "man"] instead. FEATURE:1 is a special control 
feature that means head-path.

[0 !XP 'stem'] (cf BSTEM) is to match a lexical 'stem', but not a singleton XP 'stem'.  This use is rare or not needed because 
since our lexicon can also do grammar thing, XP is not always chunked at one level, hence [0 'stem'] should be sufficient. 

Feature Update Actions

(i) feature assignment action; (ii) feature removal action; (iii) WSD feature update action; (iv) special feature assignment.

The feature update action for a token is represented by the syntax feature and feature-, for example:f1 f2 f3- f4-

In the sample code above, the two features f1 and f2 will be assigned (i.e. to set the corresponding feature bit to 1) 
and the other two features f3 and f4 will be removed (i.e. to set the corresponding feature bit to 0). For another example,

<IN NP:PP NP2 NP->;

This PP rule is to enable the HEAD token (by default it is the last token of the chunk being formed) of the newly formed phrase 
to be assigned PP and NP2 and to remove the (formerly assigned) NP feature. 

<'do|does' @not (RB:^H.R)? ^H[VB:VG Negative Simple Present]>;
== <'do|does' @not [RB:^.R]? ^[VB:VG Negative Simple Present]>;
== <'do|does' @not [RB:^.R]? [VB:VG Negative Simple Present]>;
== <'do|does' @not (RB:^VB.R)? ^VB[VB:VG Negative Simple Present]>;

<'do|does' @not [RB:^.R]? ^[VB:VG Negative Simple Present]>;
== <'do|does' @not (RB:^H.R)? ^H[VB: VG Negative Simple Present]>;

Note that a feature before : is a condition while a feature after : is an action of assigning the feature. 

Feeature Ontology Based Assigment and Removal

Each time we assign a FEATURE, the system will look up the feature ontology table to automatically assign all its parent chain feature 
along taxonomy. Moreover, it will check the incompatibility table to issue a warning in running stage for any assigned feature to conflict 
existing features (the assignment fails).  In such cases, if an explicit plus sign is used for feature assigment, then the assignment will win 
and prevail, the existing conflicting feature will be removed, again with a warning message in debug.

Each time we reemove a FEATURE, the system will look up the feature ontology table to automatically remove all its parent chain feature 
along taxonomy. 
 
POS special assignment

POS special assignment uses plus-plus sign (cf: single plus is concatenation), it actually assign a POS tag and at the same time 
remove all competing POS tags.  It is often used when context is clear.  e.g.  

[...:N++]
==[...:N V- A- R- X-]

Sentiment Removal

S--  //         delete triggerWord

In sentiment analysis, there is often a need to remove all sentiment related features, e.g. when we identify a name, any part of the name 
which has been associated with sentiment features should all be removed.  S-- is to tell the system to do so.

Binary Link Actions

(i) Pointer association; (ii) chunk internal binary link action (specified above); 
(iii) binary link actions between chunk tokens.

Most elements in a pattern do not need the action of the pointer association thanks to the pattern-element-action design. 
Pointer association, if present, is always placed as a prefix ^pointer before the token description, 
a pointer is a unique identifier or label for a token: ^pointer[...].  This is one unique "action" that is not placed after colon.  
This syntax greatly enhances the rule readability.   

Each token may be associated with an appointer ^x as a prefix before [...] so that it can be used in its child nodes for 
association of relations. For example,

{ ^VG[V] ^NP[NP:^VG.Obj] [AP:^NP.Mod] }; 
// ^NP token is associated with ^VG token as its Obj (Object); 
// the last token AP is associated with ^NP token as its Mod (Modifier) 

Dependency links can be used to help build a parse tree (more accurately, graph) of a sentence.  
Binary dependency relationships represented by the links are used as building blocks of the tree.  
Each dependency relation links a child token to its parent token with a relationship type, 
such as Subj (for Subject), Obj (for Object), Comp (for Complement), Mod (for Modifier), 
Adv (for Adverbial) and Conj (for Conjunctive).

SVO ([...:^V.Subj] ^V[...] [...:^V.Obj]) and SVOC ([...:^V.Subj] ^V[...] [...:^V.Obj] [...:^V.Comp]), 
with V as parent (the above only for illustration, there are linear variations not necessarily in that particular word order) 
are basic argument structures common across languages,  e.g. ["She":^V.Subj] ^V["gave"] ["books":^V.Obj] ["to the children":^V.Comp].
Linking in this formalism refers always to a binary dependency link which involves two elements in a pattern, one as parent 
and one as child.  Since linking involves two elements (two nodes in a tree or graph), the action is represented in the 
child element that needs a pointer for referring to the parent element.

The symbol ^ is used for a nameless pointer or in ^pointer to facilitate the linking action representation in the data structure.  
More specifically, a pointer is represented by writing a ^pointer or nameless pointer ^ immediately before the token sign [...] 
for its reference association  (e.g. ^[...], ^NP[...], ^V[...], ^Pred[...], ^PP[...], ^VP[...], ^AP[...], etc.) . 

Note that in chunking rules <...>, the nameless pointer ^ is equivalent to ^H (head pointer of the chunk, by default the last token),
but in non-chunking rules, the nameless pointer ^ is used for patterns which does not care in giving a name 
(of course, only one nameless pointer is allowed in a pattern for otherwise it is not unique.

Syntax like ^parent.Rel is a linking action for binary dependency relationship Rel between a child token (this token) 
and its ^parent token, using dot to represent the directed link. A sample linking rule for decoding adverbial (Adv) 
relationship is illustrated below: 

{ ^VG[VG] [N|A]*  RB:^VG.R }; // adverb (RB) acts as ^VG's adverbial (the dot corresponds to the possessive apostrophe)
TEST: We love our mother land dearly >> ^love our motherland dearly:^love.R

As seen, the linking action links a child token to its parent token to represent a binary dependency relationship. 
It is represented in the syntax ^parent.Relation, read as parent's relationship. 
So the appearance of a ^pointer.Rel after a dot in the action part always represents a specified binary relationship.  
In the sample rule shown above, the last adverb (RB) in the pattern is decoded to be the preceding ^VG's Adv (for adverbial).  


Macro Mechanism

Macro is a device used to save shared code in the rules.  If macros are clearly defined and the boundaries are natural and intuitive, 
their use enhances the maintainability of the code base of the grammar. Proper use of macros helps the grammar look clear 
and compact, for otherwise the code may contain too much redundancy which is not good for maintenance and debugging.  

But macros can also be easily abused to make the code less straightforward and readable for maintenance.  Embedded or recursive calling of macros 
is not advisable as it makes the patterns look different from the target surface pattern in the input data, increasing the difficulty of 
reading the code.  As a rule of thumb, most rules should use only one level of macro calls.  When a rule makes 2 levels of macros calls 
(calling a macro which calls another macro), the system should issue a warning.  An error message will appear when 3 or more levels of 
macro calls are used.

Macros are typically expanded first to expanded macro-free rules in the pre-processing stage before an FSA++ compiler works on 
parsing and compiling (building) the network from the grammar.  In theory, a system can go without macros. In practice, 
before parsing a grammar, the system performs a de-macro pre-processing step to turn macro calls into copy-n-pasted code.

Macro has two types: @PatternMacro (with prefix @) does not have action part, but @@RuleMacro (with prefix @@) can have an action part, 
making it look very much like a (parameterized) rule in format/syntax.  When calling a macro, we use prefix @ regardless: 
@PatternMacro and @RuleMacro(parameters).

Pattern Macro

PatternMacro starts with the prefixing symbol @, e.g.

@are = "are|am|im|is|\'s|seems|seem"
@not = "not|never|neither|no_longer|hardly|rarely|scarcely|barely|seldom"

@macro = 
{ 
pattern1
|pattern2
};
== 
@macro::
	pattern1
	pattern2
	
The second format ensures one line a pattern and the OR is implicit between lines without the need for "|".  
Note the diff between this format and expert lexicons.


Pattern macros are used for shared code of a sub-pattern (including token).  By convention, we name macros for literal enumeration 
by picking a representative token in lower case, e.g. @are is a literal macro for words like 'are' (i.e. are/is/am).  

Pattern macros can call other PatternMacros as long as they are instantiable and do not call more than 2 levels deep or go into circular loop of calls. 

Rule Macro

Rule macros are used to represent the shared code in the form of a parameterized rule, which is common across a set of rules.  
Some related rules are only different in small details in pattern condition and the corresponding actions, making it suitable 
or necessary for using RuleMacros for code generalization (in some European languages, morphology rules involving complex 
gender, case, number and person assignment can hardly be maintainable without the use of RuileMacro).

RuleMacro starts with the symbol @@.  The number of parameters are predefined in the RuleMacro using 1 2 3 4...,
comma is used to delimit two types of parameters.  The first type of parameters represent a sub-pattern (including a token 
description), by convention we use upper case letters naming it, prefixed by n=$, as shown in sample rules below.
The second type of parameters after comma represent a piece of code in action part, usually for a feature update,
by convention we use lower case letters naming it, prefixed m=$, as illustrated below.  

@@SimpleMPassive(1=$NEG, 2=$neg) 
=	{ 	<@modalV $NEG [RB:^.R]? "be" [RB:^.R]? [VBN|ED:VG Passive Simple Modal $neg ]> 
		[!NNS] 
	}; // using post-context to block: *these will be planned steps

From coding style perspective, RuleMacros should be kept as close to rules calling them as possible as otherwise the code 
for the calling rules is not easily readable.  The essential pattern is represented in RuleMacro rather than the calling rules.
For example, the RuleMacro @@SimpleMPassive covers both positive and negative patterns of the Simple Modal Passive verb forms:

@@SimpleMPassive(1=$NEG, 2=$neg) 
=	{ 	<@modalV $NEG [RB:^.R]? "be" [RB:^.R]? [VBN|ED:VG Passive Simple Modal $neg ]> 
		[!NNS] 
	}; // blocking: *these will be planned steps

SimpleModalPassive = @@SimpleMPassive(1,2);
TEST: They will quickly be moved out. >> will quickly:^.R be moved_out

NegSimpleModalPassive = @@SimpleMPassive(1=@not,2=Negative);
TEST: They will never be quickly moved out. >> will never be quickly:^.R moved_out

Rules call a RuleMacro by instantiation of params in the format like =@@RuleMacro(1=$PARAM1, 2=$param2), as shown above.
The negative rule NegSimpleModalPassive calls the RuleMacro @@SimpleMPassive(1=@not,2=Negative) with two params instantiated: 
the first condition param $NEG is instantiated with a literal macro call for negated adverbs like "not" (@not); 
the second action param $neg (after camma) is instantiated by the feature Negative to be assigned.

Parameters in RMacro

RuleMacros usually have two types of parameters, one for sub-pattern condition and the other for features to update in action.  
Both the condition parameters and the action parameters are represented by prefix n=$ inside (...) and they are delimited by comma.  
Between multiple parameters within condition or action parameter type, SPACE is used as delimiter, as shown below.
n denotes the number of parameters in its natural number ordering to make parameters explicit.

@@ProgressivePPassive(1=$BE 2=$NEG, 3=$tm 4=$neg)
= <$BE $NEG [RB:^.R]? "being|getting" [RB:^.R]? [VBN|ED: VG $Passive Progressive $tm $neg ]>;

When a rule calls an RuleMacro, the instantiation happens in the linear order sequence for each type of the parameters.  
For example, the following rule call the RMacro above.

NegProgressivePresentPassive = @@ProgressivePPassive(1=@are 2=@not, 3=Present 4=Negative);

In this rule, @are instantiates $BE, @not instantiates $NEG. For the action parameters, Present instantiates $tm and Negative 
instantiates $neg.  A condition param is just a piece of code for a sub-pattern (including a token description), therefore, 
a param value can be a macro call for instantiation, see @not below: 

NegSimpleModalPassive = @@SimpleMPassive(1=@not, 2=Neg);

NULL Instantiation of Params

The keyword NULL is used for instantiation of a parameter to "nothing". For example,

ProgressivePresentPassive = @@ProgressivePPassive(1=@are 2=NULL, 3=Present, 4=NULL);

NULL is often omitted together with the equal sign. So the above rule can be rewritten as shown below:

ProgressivePresentPassive = @@ProgressivePPassive(1=@are 2=NULL, 3=Present, 4=NULL);
== ProgressivePresentPassive = @@ProgressivePPassive(1=@are 2, 3=Present, 4);

@@SimpleMPassive(1=$NEG, 2=$neg) 
=	{ 	<@modalV $NEG [RB:^.R]? "be" [RB:^.R]? [VBN|ED:VG Passive Simple Modal $neg ]> 
		[!NNS] 
	}; 

SimpleModalPassive = @@SimpleMPassive(1, 2);
== SimpleModalPassive = @@SimpleMPassive(1=NULL, 2=NUL);

The rule eventually becomes after macro expansion:

SimpleModalPassive 
=	{	<@modalV [RB:^.R]? "be" [RB:^.R]? [VBN|ED:VG Passive Simple Modal ]> 
		[!NNS] 
	};

Who call whom 

RuleMacro can call PatternMcros, too. But neither PatternMacros nor RuleMacros can call RuleMacros; only rules can call RuleMacros.

Pattern Macro without Params

Simple PMacro such as literal listing macro @not has no parameters, nor do they need the parentheses syntax () for parameters, for example,
@are = "are|am|im|is|\'s|seems|seem";
Pattern Macro with Params
Some @PMacro may have parameters in the syntax like @PMacro($PARAM1,$PARAM2). For example,
(sample macros with parameters will be provided here in time)

Param Naming Convention

The convention related to naming feature $param is illustrated below.

The corresponding initial lowercase prefix (which indicates the feature type or attribute) is used as $param.  
This will make code more readable and easier to debug, e.g. $pos is param that can be instantiated by POS features such as N.  
$suf is param that can be instantiated by suffix features such as ED.  $tm is a tense/time related feature param which can be 
instantiated by features such as Past or Infinitive.　　For Feature type of misc features, the corresponding param name urns the 
Feature to lowercase, that is, it takes the form $feature.  So $neg is param for the feature Negative or NULL.

Sub-pattern for Param Instantiation

Since a param represents a piece of code for a sub-pattern, the calling rule can instantiate it by a pattern defined by (...), e.g.

SampleRule = @@RMacro(1=("will have") 2=@not, 3=Modal 4=Negative); // the first param matches a two-token sub-pattern: "will have"

Non-essential Details

[] is used to match any token. For example,

{DT []? A N} // An optional token of any type is allowed to occur after determiner and before the adjective in this pattern

Done

There is a special feature FEATURE:Done to make a token invisible in pattern matching.

Once Done is assigned to a token in a pattern or carried over from an entry in the stem (feature) lexicon, 
the system will ignore/bypass the token in subsequent pattern matching, as if it did not exist.
In implementation, the nextToken method will just jump over tokens with FEATURE:Done when the runner calls nextToken in pattern matching, 
making it invisible in effect.  This FEATURE:Done can also be assigned in lexicon to make certain token(s) disappear once they are looked up.  
This is a useful trick to get rid of some predictable language gibberish which do not have valuable semantics.

FOOT Checking

After a multi-token XP is formed, by default a token condition checks the head token of XP because non-head tokens are already bypassed.
But certain non-head tokens are still accessible, including FOOT, FIRST and LAST (it does not necessarily have to be HEAD).
If we want to check Foot token conditions of an XP, we use the syntax F. before WORD/STEM/FEATURE checking, as shown below.

[F=("in|at|on") PP Time: PPtime ]
== [FOOT=("in|at|on") Time PP: PPtime ]
== [PP Time F=("in|at|on"): PPtime ]
 // for a PP with FEATURE:Time (i.e. the HEAD token has this feature)
// also check its FOOT to be one of the three words WORD:in|at|on
TEST: it happened at 5 o'clock on an afternoon in May, 2012 >> /at 5 o'clock/ /on an afternoon/ /in May, 2012/

Note that F=("in|at|on"), the internal FOOT node checking after F= should be a token description, but we do not allow this internal 
token description to use [...] syntax to avoid the token embedding such as [...[...]...].  Instead, we use (...) for internal 
token description for ^FOOT, ^FIRST, ^LAST.  

^F is reserved for ^FOOT node just like ^H is reserved for ^HEAD node.  We also have ^FIRST and ^LAST reserved for checking leftmost token of a chunk 
and the rightmost token of the chunk.  F=(...), FIRST=(...) or LAST=(...) used inside the token description [^F=("prep")] to tell the system to 
follow the ^FOOT node, the FIRST node, or the LAST node to check the token-internal conditions.  We do not need to use ^H=(...) because by Head 
Princinple the system always follows HEAD node in checking conditions of an XP.    

[NP FIRST=(DT)]   // NP starting with determiner (DT)

[NEcompany F=(!'比'): ^.Topic] 
// a XP token of company name (NEcompany) but FOOT cannot be STEM:比, this pattern matches ^PP[在苹果公司] but not ^PP[比苹果公司]
TEST： 她在苹果公司任职 >> 在苹果公司
TEST: 谷歌比苹果公司市值低 >> ! 比苹果公司

Special feature removal functions 

++ is to assign a POS feature and also remove existing other major POS feature, i.e. -N, -V, -A, -R, -P and -X, plus all their child 
features such as NN, NNP, NNS, VB, VBD, ….  But candidate POS features N9, V9, A9, R9, X9 assigned by lexicon will remain unchanged in rules.
For POS feature taxonomy, there will be other documentation to specify.

{DT V:N++}; // after DT, the verb V is changed to N, other POS features except for features ending with -2 are all removed

XP assignment in chunking is to assign an XP, and also remove other existing major XP feature, to be replaced/assigned by XP2. 
e.g. NP --> NP2 in order to leave a track of embedded chunking info.

<P NP|DE: PP} // after P (preposition), the NP/DE is changed to NP2/DE2 and PP is assigned while chunking

Feature removal function -F

-F is to remove all removable features of a token, this is an important action to prevent a disambiguated token from side effects 
of the original lexical semantic features. All features are removable unless listed in feature exception list (to be lexically specified 
in time). Un-removable exception feature list includes features like OCase (orthographic case) features and length-of-token string features.

VG Rule Macros as Illustration

Many VG Rule Macros cover both positive and negative patterns.

The positive rule calls it with no param $NEG instantiation (i.e. 2=NULL, or just 2) and no feature (4=NULL or just 4) 
instantiation for update (i.e. NULL). The negative rule calls it usually with $NEG instantiation to @not and $neg instantiation 
to Neg for feature update.　The negative rule is named with the prefix Neg-, e.g. NegPerfectPresent() below:

@@PerfectT(1=$HAVE 2=$NEG, 3=$tm 4=$neg) 
= <$HAVE $NEG [RB:^.R]? [VBN|ED:VG Perfect $tm $neg ]>; // we have/had (never) visited the city

PerfectPresent = @@PerfectT(1=@has 2, 3=Present 4);
==PerfectPresent = @@PerfectT(1=@has 2=NULL, 3=Present 4=NULL);

NegPerfectPresent = @@PerfectT(1=@has 2=@not, 3=Present 4=Negative);


Unit Test in Rule

Unit Test (TEST) is illustrated in the following format after each rule:

SampleRule = <"a unit" ["test":NP]>;
TEST: This is a unit test. >> NP[a unit test]

TEST is not part of rule body, involving no compiling and running. It is to record Unit Test input and expected output. 
It can be treated as a new type of comment in the rule parsing.

At least one TEST must accompany each rule, otherwise the parser should issue a warning message.
Following TEST: are the unit test input and output connected by >>, the input is a real life token string that can be 
cut and paste to feed the system for unit testing.　The output takes some form that should be defined in time to conform 
with the system output from that module so an automatic utility can becalled to check whether it is unit-tested properly.
In future, a utility will be developed to automatically verify the unit test cases periodically and issue warning messages. 
For failed unit tests, the utility will warn the developer and record it in the rule in some form for the developer to examine 
it in time.

Comments in Rules

One line comments are placed after // and any lines of comments can be placed inside /* ...... */.

/* An FSA++ rule is based on pattern matching.
The simplest rule is enumeration of the token literal in sequence, as shown below.
*/

SampleRule = {"this" "is" "a" "test"}; // this sample FSA++ rule macthes the input string "this is a test".



============================================================================================
Notes (to be done/s[ecified):

1. revise existing ontology nodes: rename and select a subset to start with and add more as needed;

2. regex

letter-based regex can be used in checking a token's prefix or sufffix etc.  This device helps analysis of open morphology.

Suffix Checking

The system supports substring matching of a token, starting with prefix and suffix.
"-..." is used for suffix checking and "...-" is used for prefix checking.

["-ed|-en"]  // match words ending with -ed or -en
["co-"]      // match words beginning with co-

3. inXP= Checking:  traverse a chunk XP, and check whether any token inside XP meets the condition:

[inXP=(!"word")...]
[inXP=(!feature)...]

4.　ngram traverse

["to the children":^V.Comp] vs ("to the children")

("to the children")              // this is a pattern
 == ("to" "the" "children")
 == (["to"] ["the"] ["children"]) 
 
["to the children"]				// this is an ngram token traverse of chunked phrase leaf nodes 
== ["to" "the" "children"] 
== [{ ["to"] ["the"] ["children"] }]

Traversing can go beyond literal ngram tokens using token-internal patterns using syntax [{...}]: 

[{"to" DT 'child'}]
== 	[{ ["to"] [DT] ['child'] }]

Note: just like <<...>>, in [{...}], the "[{" and "}]" are atomic reserved symbols in FSA++, space is not allowed in between.

5. concatenation

(个+人:^.mm)
== (个+[人:^.mm])
==(<['个':Done+] ['人':^.mm '个人'+ /个人/+]>)

We allow both forms with the last token as default host for the new open compound:

[f1+f2] == [f1]+[f2]

6. lexicon key: stem a / stem n / stem v /...; we add more keys for lexical support for WSD: stem1 / stem 2 / stem 3 ...
Note that sense without a subscript digit by default may mix features of different senses together; 
once WSD-ed, rules or systems can update features by using stem+digit lookup.

7 Reset 

The use of `[...] is to tell the system to Reset the move-head at the point for the next round of pattern matching.
Note that the symbol is on the key with "~ and `" under the key Esc in the keyboard.  

If the reset token is optional, then the system will automatically decide to reset to the next available token if the optional 
token is not present in the pattern matching.  This way the Reset Next support becomes unnecessary.  It also enhances the readability.

旗下:: 
	[旗下:A++ modJJ leftB-] `[NN|最]
安利:: 
	[安利:V++ oralW orgN- brandN- name- objectW-] `[human]

V_Space_O4 = ^[VG !vi|passive|perfect|progressive Kid=!Buyu|O|Dummy] ['%space'] [NP|DE Dad=!O triggerWord perOrgPN:^.O2] `[RB|TO]? [!RB|TO|AP|VG]


8. automatic blocking of double parenting (eliminating PARENT: None)

by default, it automatically will not allow for doiuble parenting (with no need to add PARENT: None constraint):
block the second attempt to link a child which already has a parent with a second parent.

if we want to do it on purpose, it should be made explicit in some way

9. Kid=(...) and Dad=(...)

this is to check the linked relationship from its child (or from parent). 

把_S2_de_NN2 =
{	['把']
	[NP|PRP|N: ^V.Subj] 
		[PP|EX:^V.R]?
	( ^V[VB !beW|haveW ^KID=(!Subj): ^N.RelMod] | ^V['要': ^N.RelMod] )
	[0 '的|之':^N.Dummy]
	^N[NP|DE !'时候|同时'|PRP]
};

10:     [IN|CC #ASET=PrepAdjSet]

11. Expert Lexicon Design

Two types of expert lexicons: (a) first is indexed expert lexicon and (b) second is non-indexed lexcalized grammar.

11a. Indexing stem with basically the same format except for the possible use of ~~ (== 'stem') to represent the indexing stem.
By default, ~~ checks STEM:stem, hence ~~ == 'stem'; in cases of WORD checking, use "word". for readability, we tend to repeat 
the index stem instead of using ~~ because the system can associate it inside expert lexicons by string matching if needed
(knowing where ~~ is inside an expert lexicon pattern can optimize the matching effficiency once long patterns are used).

The indexing item is before :: and the rule brackests {...} and semicolon are omitted:

高兴:: ^~~ [NP !postP:^.O] 
== 高兴(Top) = { ^'高兴' [NP !postP:^.O] };   

If there are more entries of that indexed item, use th format like

stem::
... ~~ ...
... ~~ ...

In implementing expert lexicons, the efficiency can be optimized by indexing techniques to decide which lexicalized module to call or not to call. 
One extreme is to compile an expert lexicon at certain pipeline level into one FSA module.  Another extreme is to compile 
rules for each index entry of a certain pipeline level lexicon as a tiny FSA mini-module.  
In practice, implementing engineers always choose somewhere in between for optimized result. 

11b. non-indexed expert lexicon 

it is in principle no different from grammar except that we do not bother giving a rule name to it (rule name is optional) and they are more lexicalized 
so we want to keep them away from generlaized grammar rules.

one line per rule so we can omit {...};

line ordering is kept for tracking purposes (due to no rule names).

11c. WSD or contextual feature update using expert lexicon

(...<<TOken:f1 f2->>...) in an early stage Expert Lexicon is an easy way to update features (or lightweight WSD) based on context 
without interfering with normal parsing. Particularly useful in domain specialization when the core parser remains the same.
For example, in sentiment extraction, a special expert lexicon dedicated to updating the sentiment features will greatly enhance quality. e.g. 

stop <<"carrying":useW>>
<<cool:pW->> part
no <<confirmation|reply|response:comfortW>> to me
<<take:useW>> chemicalN|drugN for diseaseN
<<give:pW->> me the? [] "reading"
<<apply:pW->> !for
please do not <<nW:comfortW nW->>
<<"stolen":nW->> by
 
12. NEW action and NEUTRAL

NEW f1 f2 
NEW is to wipe out all wipable features (not all features are wipable, some otho features like length of token, 
capitalization, cannot be wiped: these features are in a configurable exception-list for the system to access.
in implementation they may be placed in an "unwritable" area of data structure).  So f1 f2 ... will be added 
to the feature table. 

酒::
	<酒+过+CD+巡:RP NEW idiom4 timeRB advP> 
	
NEUTRAL is to remove all sentiment (pW and nW etc.) related features so it won't get sentiment tagging triggered.

13. pseudo-idiom is like a blackbox even if it is productive

select the last token as the representative of the idiom in idiom rule, (i) use NEW to reduce side effect;
(ii) token length update to the entire string; (iii)

14. Spcial 0 feature : 

0 (Xbar0) is a so-called configuration special feature (from X-bar theory), which is given to all wwords, 
including an open-ended compound word, to distinguish it from a phrase XP. 

When concatenating chunking action is used for open-ended compounding, the compound is considered as a lexical entry 
instead of a syntactic combination.  The system has a few corresponding things to take care of internally. e.g.

<[暗|鲜|淡|深:^m]+color:A++>

Note the concatanation chunking syntax in the form <stem1+stem2>. Normal chunking forms XP, but concatenating chunking 
does not: there are a list of diffs between this special chunk from [XP] chunk.

The goal is to make lexical entry and the open compounding of the same syntactic configuration class, checkable by [0 ...].
Other things include "concatenating into one word and make the non-head morpheme gone (or done, or disappear).

15. STEM and NORM update actions 

:'stem'   // this is for STEM update
:/norm/   // this is for NORM update : it happens when WSD is done and needs to put it into another NORM cluster

16.  Lexicon samples

股份转让方: personOrgN  human  NN  股权转让方++  // note that 股权转让方+ == '股权转让方'++ 

约有: advV  NOTaspectW  xiaociW  exist  own  transV  haveW  vacuousW  NOTobjectW  VB  /有/ // lexical NORM assignment
帝都: place  oralW  cityPN  NNP  /北京/ 
魔都: place  oralW  cityPN  NNP  /上海/ 
妖都: place  oralW  cityPN  NNP  /广州/ 



default lexicon is used as a last pass of segmentation so those "loose" words will be picked up 
only after competing words are tried.

17. reverse link 

every binary dependency link corresponds to a reverse link represented by prefixing symbol ~Link

~S 
~Obj
~Kid

18. ignoring R* can become part of system operation so the rule does not need to explicit write this

if the next token is not matched, and the next token is R, then jump over it to try the next

Since this is system level jumping over, we limit R to R|RP which does not include time|date etc..

19. linear and tree mixed formalism support

This support helps a linear pattern to check a token's subtree structure.  It is beneficial because 
our parsing is multi-level and incremental so some bottom-up sub-structures are already built.
For example, when a clause is decoded with internal SVO sub-structure and CL (Clause) feature assigned 
to a clause-token, the next module may need to check this substructure. e.g.

20. unlink support

Unlink support is important to fix errors when conditions are more ready.

^T1[] ^T2[:^T1-] // ^T1- is to cut off all links to the parent node ^T1
^T1[] ^T2[:^T1.S-]  // ^T1.S- is to delink the specified S link to ^T1

21. Done, Gone and !JS2

We have two special features Done and Gone: FEATURE:Gone is to make it disappear completely; Feature:Done is still there, but marks as already processed.

When a token is marked as Done (e.g. [0 that:Done JS2]), it itself will not need to be processed, but it can still be there 
unless specifically asked to ignore.

22. Reduplication and Unification

This is an extension in our FSA++ formalism which is not available in classical FSA.  It is a very useful device in NLP.

: '^previousstem' is to reduplicate STEM
: "^previousword" is to redupicate WORD

e.g.
来::    // 琢磨来琢磨去 = 老在琢磨
	<^H[0 c2 V|VBp:VG] 来  CM? '^H' 去>


: [^token(f1|f2|f3)] is to unifiy f1 OR unify f2 OR unify f3 respectively of the previousely mentioned ^token

For example, to check whether both tokens have human, company, location.  This is very useful for patterns of conjunctive structures. 

23. Embedded Chunking and ^H2 

The formalism suports embedded chunking in one pattern in the form of <....<...> ...>. e.g.

排名::  	//  排名前三名
	< ^H[排名:VG vp pW] <前|头 ^H2[CD:NP ^H.o] [名|位|个:Gone]?> >
 
The above expert lexicon pattern rule says: (i) there is an internal NP which needs to be chunked first with the HEAD pointer to be ^H2; 
(ii) the external chunk is VG with the HEAD pointer ^H; (iii) NP is the VG's Object.  

Of course, we can also avoid the above embedding by the following pattern: 

排名::  	//  排名前三名
	<^[排名:VG vp pW]> <前|头 ^H[CD:NP ^H2.O] [名|位|个:Gone]?>

From DG perspective, they are the same (except that o is internal morphology object while O is the external syntactic object), 
and we would prefer and promote the unembedded version of linguistic representation. Nevertheless, we decide to support 
the embedded-chunking as part of formalism because this gives flexibility in handling open compounding.  This case might 
go either way, but there are open compounding idiom cases (especially for idiom4) that we prefer to chunk and concatenate 
but still prefer to uncover the internal relationships of morphemes. 

24 Priority inside Ecpert Lexicon rules:

The optional (Top) = and (Bot)= can be used inside a list of rules under an expert lexicon entry to show the priority.
If there is a need to ensure one rule is of more priority of another, the same syntax of a rule naming syntax and its 
associated priority can be adopted.  e.g.

多::
	1(>2)=	{[有:Gone]? <[多:^.r wh] [长|宽|高|短|窄|低|矮:AP howAQ length]> ([的:Gone]? [大小|样子|尺寸|尺码:Done])?};
	2=		{[有:Gone]? <[多:^.r wh] [远|近|长|短|大|小:AP howAQ distanace]> ([的:Gone]? [距离|路|路程|车程|行程|样子:Done])?};
	[有:Gone]? <[多:^.r wh] [热|冷|寒冷|炎热:AP howAQ distanace]> ([的:Gone]? [温度|样子|度数:Done])?
	[有:Gone]? <[多:^.r wh] [0 tightness:AP howAQ]> ([的:Gone]? [样子:Done])?
	[有:Gone]? <[多:^.r wh] [0 quality:AP howAQ]> ([的:Gone]? [样子:Done])?  // 有多_好 == 怎么样， quality，好坏
	(Bot)=[有:Gone]? <[多:^.r wh] [0 c1|c2 A valueUp|valueDown !xpW|idiom|xiaociW:AP howAQ]> ([的:Gone]? [样子:Done])? 

25. Collocation Prepositions and Postpositions

Many verbs (or adjective or sometimes nouns) subcats for (a set of) prepositions, this is a huge topic that needs more refinement 
as time goes.  The design idea is:

set a specific collocation lexicon in the following format:

// prep collocation lexicon
服务：：
为|替NP ^服务: V++ NEW f1 f2   // once saturated, we know it is V and its sense and feature may need update too
^服务 于 NP: V++ NEW f1 f1

26. Spurious embedded chunking and spurious linking

If a chunk named XP is rechunked with the same type of XP and the same scope, the system simply does not need the embedded chunking,
basically doing nothing.  Embedded chunking only applies when the type or the scope of the chunking is different.

Same goes with spurious linking when a new link between two nodes is the same link type with thw same two nodes.  The system will 
do notthing.  

27. chunking (or some parsing) not allowed to cross Oqut and Cqut

This type of control will be automatically done by the system with incremental fine-tuning.  Rules' level control is very difficult
(handles one case and may forget many other cases: by nature this is system level thing) and it makes code look ugly too.  For example,
things like the following should not be needed inside rules:

( [N !Oqut !Cqut] | [N Oqut Cqut] )

[N] is fine; ['N'] is also fine; but ['N] N2' should not match in [...] chunking because the first N is Oqut only without Cqut. 

28 special boundary features

In many cases, especially in 框式结构，there are clear clues where an linguistic unit begins (left boundary) and ends (right boundary).
So we use some special features to help represent the boundaries so the subsequent parsing (chunking and linking) will be more robust 
in capturing these units.

oCL 	// opening or left boundary of a clause
cCL		// closing or right boundary of a clause
oNP		// opening or left boundary of an NP
cNP 	// closing or right boundary of an NP
oMod	// opening or left boundary of a Modifier phrase (mostly AP, but not necessarily) 
cMod	// closing or right boundary of a Modifier phrase (mostly AP, but not necessarily) 

It is possible that some clear clues for left boundary only exist, or vice versa.  In such cases, we may not have the above features
in pairs.  But the boundary info is still useful.

29 mechanism to call parser n times for the same sentence

This mechanism in effect introduces top-down control or parsing strategy into the core bottom-up parsing frame.  
It is especially useful for irect speech, clauses in parentheses, a wide variety of 框式结构

The design is as follows:

(i) pre-Core top-down control to identify embedded substrings: given an input string of words for processing, we have a top down control set of rules 
and/or java procedures to decide whether this input needs to be decomposed into sub-strings for parsing.

(ii) Core parsing (same for all) of embedded substrings: for each substring the top-down control logic decides to be parsed independently, 
this substring is structured as an independent mini-sentence to call the parser from end to end.  We start with assumption such substrings 
have not embedded sub-substrings (in future depending on needs, we may allow for 1 ot 2 more levels of embedding in top-down control, 
which needs some more coordination in the parsing control.

(iii) post-Core patching of embedded substrings: after a substring is processed by the core parser end-to-end, we apply a module to patch 
this substring if needed.  The point of this patching module is to help make parsing more robust based on the hint from the top down expectation 
from (i): many times, the top down expectation has wider evidence for what this internal substring is: a clause, a modifier, an NP, etc.  
With that in mind, the patching module can do something to satisfy the expectation.

(iv) Core parsing of the core of the input: After all substrings are independently processed one by one, we need to call the parser for the 
core input string making the processed substrings technically "disappear".

This is the design sketched.  More details will be worked out when getting to implementation details.

30 Expert Lexicon Indexing 

Expert Lexicons can be indexed to enhance efficiency.  This is mainly for efficiency optimization.  The details of how to index can be 
worked out with careful consideration of time and space costs.

An extreme way of indexing is to compile each expert lexicon into one FSA, which is to be called by an indexed lexicons which directly 
point to the address of this FSA to be called at designated level by the system when the input contain the indexed word.  Then we may 
end up with tens of thousands of mini-FSAs.

The other end of extreme indexing is to index each level of expert lexicon into one big FSA, but the expected indexing efficiency will 
almost disappear when the expert lexicon contains too many indexed words.

Other ways of indexing can cut an expert lexicon into segments so that we call a segment if the input contains a word X which happens to 
be inside the segment of the indexed words.  What is the optimal size of a segment so we maximally balance the indexed efficiency and cost 
of file maintenance and segmenting. 

Finally, whether we should index STEM (or WORD) or NORM (/NORM/) is also a design choice.  The default indexing should be STEM.
But considering /NORM/ indexing might also be a good idea: of course inside the expert lexicon rules, the use of STEM, WORD or NORM
only depends on the rules, there is no restriction.

31. Lexical Feature Update from Ontology Lexicon: NEW.O

一个　lexical feature 的转移问题及其解决途径: 譬如　所+vt (.g. 所吃)，在小词　“所”　与　vt 　结合成合成词以后,该词的语义等价于vt标配宾语的语义。　换句话说　“吃”　的
动作性　[consume]　语义本体标注，应该被置换成名词性标注　[food] or [edible]. 如果　feature 不更新，那些细线条的指望语义　features 查询条件的规则就会失效。
“所”　是一例，“的字结构”　也是类似的例子。可以为每一种主要关系（譬如　动宾）从ntology取出一个语义词表，等到需要更新的时候，就让系统根据关系的规定来查表解决。

所::
	<所 [V !vi c1:NP vi NEW.O]> [!的 ^post]

NEW.O tells the system to remove the existing wipable lexical features, and lookup THIS token's default logical object feature.  
If THIS token is 吃，then it will retrieve [edible], the default object of 吃, from ontology lexicon and assigns to the updated word.

32. Three levels of Rules

(i) grammars: general feature-based rules

(ii) lexicalized rules: non-indexed word-driven rules: format is the same as (i) except that every rule stays in on line and each rule 
has at least one element to check WORD, STEM or NORM. In principle, all (ii) rules can be treated as (i) grammar, but we separate these 
files for easier maintenance and debugging, and we run them according to (ii) > (i) priority.

(iii) Expert Lexicons: indexed word-driven rules

The default priority for running the above rules are: (iii) > (ii) > (i).

33.  Support of \regex\ plugin for open morphology etc.

There is standard regex the system can call to help support character-based pattern matching for morphology: e.g. 

money_NT(Top) = <[全价|半价:^.M]? ["\^nt.[0-9][0-9]*$\":moneyDE N predicative buyuW]>

Note that the notation diff between /NORM/ and \regex\.  Also note that \regex\ is usually used inside "WORD" or 'STEM'.  
It is a more flexible way than simple prefixing "pref-" and suffixing "-suf".



============================================================================================
